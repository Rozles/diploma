{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot year graph\n",
    "\n",
    "with open('all_data_new_2.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "min_year = 3000\n",
    "max_year = 0\n",
    "for id_ in data:\n",
    "    painting = data[id_]\n",
    "    if 'completitionYear' in painting and painting['completitionYear'] != None:\n",
    "        year = int(painting['completitionYear'])\n",
    "        if year < min_year:\n",
    "            min_year = year\n",
    "        \n",
    "        if year > max_year:\n",
    "            max_year = year\n",
    "\n",
    "        if (year == -350):\n",
    "            print(painting['title'])\n",
    "            image = load_np_image(painting)\n",
    "            plt.imshow(image)\n",
    "            plt.show()\n",
    "\n",
    "print(min_year, max_year)\n",
    "\n",
    "year_array=np.zeros(max_year - min_year + 1)\n",
    "for id_ in data:\n",
    "    painting = data[id_]\n",
    "    if 'completitionYear' in painting and painting['completitionYear'] != None:\n",
    "        year = int(painting['completitionYear'])\n",
    "        year_array[year - min_year] += 1\n",
    "\n",
    "# plot bar plot with years on x axis and number of paintings on y axis\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(np.arange(min_year, max_year + 1, 1), year_array)\n",
    "plt.xlabel('Leto')\n",
    "plt.ylabel('Število slik')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from json file\n",
    "with open('all_data_new_2.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "MIN_YEAR = 1072\n",
    "MAX_YEAR = 2014\n",
    "dicti = {}\n",
    "missing = 0\n",
    "year_array=np.zeros(MAX_YEAR-MIN_YEAR + 1)\n",
    "for idd in data:\n",
    "    painting = data[idd]\n",
    "    if len(dicti) == 0:\n",
    "        print(painting)\n",
    "    try:\n",
    "        styles = painting[\"styles\"]\n",
    "        for style in styles:\n",
    "            dicti[style] = dicti.get(style, 0) + 1\n",
    "    except KeyError:\n",
    "        print(painting)\n",
    "print(len(dicti))\n",
    "styles = dicti.keys()\n",
    "print(styles)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_styles = [\n",
    "    'Byzantine',\n",
    "    'Gothic',\n",
    "    'International Gothic',\n",
    "    'Proto Renaissance',\n",
    "    'Early Renaissance',\n",
    "    'High Renaissance',\n",
    "    'Mannerism (Late Renaissance)',\n",
    "    'Northern Renaissance',\n",
    "    'Baroque',\n",
    "    'Rococo',\n",
    "    'Neoclassicism',\n",
    "    'Romanticism',\n",
    "    'Realism',\n",
    "    'Naturalism',\n",
    "    'Academicism',\n",
    "    'Impressionism',\n",
    "    'Post-Impressionism',\n",
    "    'Neo-Impressionism',\n",
    "    'Pointillism',\n",
    "    'Symbolism',\n",
    "    'Art Nouveau (Modern)',\n",
    "    'Fauvism',\n",
    "    'Expressionism',\n",
    "    'Cubism',\n",
    "    'Analytical Cubism',\n",
    "    'Synthetic Cubism',\n",
    "    'Futurism',\n",
    "    'Dada',\n",
    "    'Surrealism',\n",
    "    'Constructivism',\n",
    "    'Abstract Art',\n",
    "    'Neoplasticism',\n",
    "    'Suprematism',\n",
    "    'Social Realism',\n",
    "    'Regionalism',\n",
    "    'Precisionism',\n",
    "    'Tenebrism',\n",
    "    'Orphism',\n",
    "    'Art Deco',\n",
    "    'New Realism',\n",
    "    'Abstract Expressionism',\n",
    "    'Color Field Painting',\n",
    "    'Minimalism',\n",
    "    'Pop Art',\n",
    "    'Op Art',\n",
    "    'Conceptual Art',\n",
    "    'Photorealism',\n",
    "    'Neo-Expressionism',\n",
    "    'Street art',\n",
    "    'Cubo-Futurism',\n",
    "    'Luminism',\n",
    "    'Muralism',\n",
    "    'Moscow school of icon painting',\n",
    "    'Tonalism',\n",
    "    'Magic Realism',\n",
    "    'Neo-Suprematism',\n",
    "    'Neo-Pop Art',\n",
    "    'Transautomatism',\n",
    "    'Analytical\\xa0Realism',\n",
    "    'Excessivism',\n",
    "    'Kitsch',\n",
    "    'Art Brut',\n",
    "    'Outsider art',\n",
    "    'Naïve Art (Primitivism)',\n",
    "    'Cloisonnism',\n",
    "    'Intimism',\n",
    "    'Biedermeier',\n",
    "    'Socialist Realism',\n",
    "    'Fantasy Art',\n",
    "    'Divisionism',\n",
    "    'Kinetic Art',\n",
    "    'Neo-Figurative Art',\n",
    "    'Neo-Rococo',\n",
    "    'Feminist Art',\n",
    "    'New European Painting',\n",
    "    'Fantastic Realism',\n",
    "    'Ink and wash painting',\n",
    "    'Hard Edge Painting',\n",
    "    'Post-Minimalism',\n",
    "    'Sōsaku hanga',\n",
    "    'Tachisme',\n",
    "    'P&D (Pattern and Decoration)',\n",
    "    'Shin-hanga',\n",
    "    'Lettrism',\n",
    "    'Hyper-Realism',\n",
    "    'Lyrical Abstraction',\n",
    "    'Light and Space',\n",
    "    'Spectralism',\n",
    "    'Art Informel',\n",
    "    'Neo-Romanticism',\n",
    "    'Stuckism',\n",
    "    'Post-Painterly Abstraction',\n",
    "    'Neo-baroque',\n",
    "    'Environmental (Land) Art',\n",
    "    'Zen',\n",
    "    'Automatic Painting',\n",
    "    'Maximalism',\n",
    "    'Cartographic Art',\n",
    "    'Neo-Concretism',\n",
    "    'Toyism',\n",
    "    'Neo-Orthodoxism',\n",
    "    'Nas-Taliq',\n",
    "    'Timurid Period',\n",
    "    'Neo-Minimalism',\n",
    "    'Mail Art',\n",
    "    'Ilkhanid',\n",
    "    'Mughal',\n",
    "    'Cyber Art',\n",
    "    'Yamato-e',\n",
    "    'Concretism',\n",
    "    'Gongbi',\n",
    "    'Neo-Geo',\n",
    "    'Poster Art Realism',\n",
    "    'Neo-Dada',\n",
    "    'Nouveau Réalisme',\n",
    "    'Modernismo',\n",
    "    'Nihonga',\n",
    "    'Figurative Expressionism',\n",
    "    'Cubo-Expressionism',\n",
    "    'Superflat',\n",
    "    'Synchromism',\n",
    "    'Safavid Period',\n",
    "    'Nanga (Bunjinga)',\n",
    "    'Sky Art',\n",
    "    'Joseon Dynasty',\n",
    "    'American Realism',\n",
    "    'Hyper-Mannerism (Anachronism)',\n",
    "    'Ottoman Period',\n",
    "    'Pictorialism',\n",
    "    'Indian Space painting',\n",
    "    'Spatialism',\n",
    "    'Existential Art',\n",
    "    'Action painting',\n",
    "    'Costumbrismo',\n",
    "    'Purism',\n",
    "    'Junk Art',\n",
    "    'Street Photography',\n",
    "    'Transavantgarde',\n",
    "    'Perceptism',\n",
    "    'New Casualism',\n",
    "    'Native Art',\n",
    "    'Classicism',\n",
    "    'Ukiyo-e',\n",
    "    'Japonism',\n",
    "    'Metaphysical art',\n",
    "    'Orientalism',\n",
    "    'Synthetism',\n",
    "    'Contemporary Realism',\n",
    "    'Lowbrow Art',\n",
    "    'Verism',\n",
    "]\n",
    "\n",
    "print(len(ordered_styles))\n",
    "\n",
    "for style in dicti:\n",
    "    if style not in ordered_styles:\n",
    "        print(style)\n",
    "\n",
    "ordered_styles_dict = {}\n",
    "for style in ordered_styles:\n",
    "    ordered_styles_dict[style] = dicti[style]\n",
    "\n",
    "  \n",
    "# y_limit = np.max(list(ordered_styles_dict.values())) + 50\n",
    "# plt.figure(figsize=(7, 7))\n",
    "# plt.bar(list(ordered_styles_dict.keys())[:50], list(ordered_styles_dict.values())[:50])\n",
    "# plt.xticks(rotation=90, fontsize=8)\n",
    "# plt.ylabel('Stevilo slik')\n",
    "# plt.ylim(0, y_limit)\n",
    "# plt.savefig('slike/stevilo_slik_style_1.png', bbox_inches='tight', dpi=120)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(7, 7))\n",
    "# plt.bar(list(ordered_styles_dict.keys())[50:100], list(ordered_styles_dict.values())[50:100])\n",
    "# plt.xticks(rotation=90, fontsize=8)\n",
    "# plt.ylim(0, y_limit)\n",
    "# plt.ylabel('Stevilo slik')\n",
    "# plt.savefig('slike/stevilo_slik_style_2.png', bbox_inches='tight', dpi=120)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(7, 7))\n",
    "# plt.bar(list(ordered_styles_dict.keys())[100:], list(ordered_styles_dict.values())[100:])\n",
    "# plt.xticks(rotation=90, fontsize=8)\n",
    "# plt.ylim(0, y_limit)\n",
    "# plt.ylabel('Stevilo slik')\n",
    "# plt.savefig('slike/stevilo_slik_style_3.png', bbox_inches='tight', dpi=120)\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 13), dpi=200)\n",
    "plt.barh(list(ordered_styles_dict.keys()), list(ordered_styles_dict.values()), height= 0.8)\n",
    "plt.yticks(fontsize=6)\n",
    "plt.xticks(fontsize=6)\n",
    "plt.subplots_adjust(left=0.15, right=0.95, top=0.95, bottom=0.05)\n",
    "plt.xlabel('Število slik', fontsize=8)\n",
    "plt.gca().tick_params(axis='y', labelrotation=0, labelleft=True, labelright=False, labelsize=6)\n",
    "plt.tight_layout()\n",
    "plt.savefig('slike/stevilo_slik_style.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot = cv2.imread('slike/stevilo_slik_style.png')\n",
    "\n",
    "cut_pixels = 100\n",
    "padding_top = 20\n",
    "padding_bottom = 90\n",
    "result_img = 255 * np.ones(shape=[plot.shape[0] - 2 * cut_pixels, plot.shape[1], 3], dtype=np.uint8)\n",
    "\n",
    "# Place the images side by side on the new array\n",
    "result_img[0:padding_top, :,:] = plot[0:padding_top, :]\n",
    "result_img[padding_top:-padding_bottom, :,:] = plot[cut_pixels + padding_top:-(padding_bottom + cut_pixels), :, :]\n",
    "result_img[-padding_bottom:, :, :] = plot[-padding_bottom:, :,:]\n",
    "\n",
    "# Save the result\n",
    "cv2.imwrite('slike/graf_test.png', result_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8691/8691 [29:21<00:00,  4.93it/s]  \n"
     ]
    }
   ],
   "source": [
    "# SAVE 8 x 8 IMAGE\n",
    "\n",
    "with open('all_data_depth.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "bar = tqdm(total=len(data))\n",
    "for painting in data.values():\n",
    "    if 'depth_map' in painting:\n",
    "        bar.update(1)\n",
    "        continue\n",
    "    try:\n",
    "        image = load_np_image_2(painting)\n",
    "        depth_map = predict_depth(image)\n",
    "        depth_map = ((depth_map - np.min(depth_map)) / (np.max(depth_map) - np.min(depth_map)) * 255).astype(np.uint8)\n",
    "        eight_by_eight = cv2.resize(depth_map, (8, 8))\n",
    "        painting['depth_map'] = str(eight_by_eight)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    bar.update(1)\n",
    "\n",
    "bar.close()\n",
    "with open('all_data_depth.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/561 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 561/561 [33:36<00:00,  3.59s/it]\n"
     ]
    }
   ],
   "source": [
    "# TEST DIFFERENT DEPTH MAP SIZES\n",
    "\n",
    "with open('data_backup.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "bar = tqdm(total=len(data))\n",
    "for painting in data.values():\n",
    "    try: \n",
    "        image = load_np_image(painting)\n",
    "        depth_map = predict_depth(image)\n",
    "        depth_map = ((depth_map - np.min(depth_map)) / (np.max(depth_map) - np.min(depth_map)) * 255).astype(np.uint8)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "    if 'depth_eight' not in painting:\n",
    "        eight_by_eight = cv2.resize(depth_map, (8, 8))\n",
    "        painting['depth_eight'] = str(eight_by_eight).replace('   ', ' ').replace('  ', ' ').replace('[ ', '').replace('\\n', '').replace('[', '').replace(']', '')\n",
    "    \n",
    "    if 'depth_sixteen' not in painting:\n",
    "        sixteen_by_sixteen = cv2.resize(depth_map, (16, 16))\n",
    "        painting['depth_sixteen'] = str(sixteen_by_sixteen).replace('   ', ' ').replace('  ', ' ').replace('[ ', '').replace('\\n', '').replace('[', '').replace(']', '')\n",
    "    \n",
    "    if 'depth_thirtytwo' not in painting:\n",
    "        thirtytwo_by_thirtytwo = cv2.resize(depth_map, (32, 32))\n",
    "        painting['depth_thirtytwo'] = str(thirtytwo_by_thirtytwo).replace('   ', ' ').replace('  ', ' ').replace('[ ', '').replace('\\n', '').replace('[', '').replace(']', '')\n",
    "    \n",
    "    bar.update(1)\n",
    "\n",
    "bar.close()\n",
    "with open('test.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLIFY DATA\n",
    "\n",
    "with open('all_data_depth.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for painting in data.values():\n",
    "    painting['depth_map'] = painting['depth_map'].replace('   ', ' ').replace('  ', ' ').replace('[ ', '').replace('\\n', '').replace('[', '').replace(']', '')\n",
    "    \n",
    "with open('all_data_depth.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5623/5623 [4:05:43<00:00,  2.62s/it]      \n"
     ]
    }
   ],
   "source": [
    "# IMAGE HASH *SAVE HASHES TO JSON FILE*\n",
    "\n",
    "file_path = 'all_data.json'\n",
    "\n",
    "with open('all_data_4233.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "bar = tqdm(total=len(data))\n",
    "for idd in data.keys():\n",
    "    if 'depth_hash' in data[idd]:\n",
    "        bar.update(1)\n",
    "        continue\n",
    "    image = load_np_image(data[idd])\n",
    "    depth_map = predict_depth(image)\n",
    "    image_hash = imagehash.average_hash(Image.fromarray(depth_map))\n",
    "    data[idd]['depth_hash'] = str(image_hash)\n",
    "    bar.update(1)\n",
    "\n",
    "with open(file_path, 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "\n",
    "bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE HASH *COMPARE HASHES*\n",
    "\n",
    "with open('all_data.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "key_list = list(data.keys())\n",
    "for i in range(0, len(key_list)):\n",
    "    min_diff = 64\n",
    "    for j in range(0, len(key_list)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        hash1 = data[key_list[i]]['depth_hash']\n",
    "        hash2 = data[key_list[j]]['depth_hash']\n",
    "        diff = imagehash.hex_to_hash(hash1) - imagehash.hex_to_hash(hash2)\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            closest = key_list[j]\n",
    "    \n",
    "\n",
    "    hash1 = data[key_list[i]]['depth_hash']\n",
    "    hash2 = data[closest]['depth_hash']\n",
    "    diff = imagehash.hex_to_hash(hash1) - imagehash.hex_to_hash(hash2)\n",
    "    print(hash1, hash2, diff)\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(load_np_image(data[key_list[i]]))\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.imshow(load_np_image(data[closest]))\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.imshow(cv2.resize(predict_depth(load_np_image(data[key_list[i]])), (8, 8), interpolation=cv2.INTER_AREA))\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(cv2.resize(predict_depth(load_np_image(data[closest])), (8, 8), interpolation=cv2.INTER_AREA))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_plane_with_threshold(im, depth_map, sigma):\n",
    "    print('shape', depth_map.shape)\n",
    "    print(depth_map.shape[0] * depth_map.shape[1])\n",
    "    rangee = np.max(depth_map) - np.min(depth_map)\n",
    "    hist, bins = np.histogram(depth_map, bins=int(rangee))\n",
    "\n",
    "    kernel = gauss(sigma)\n",
    "\n",
    "    smooth = np.convolve(hist, kernel, mode='same')\n",
    "    diff = np.roll(smooth, -1) - smooth \n",
    "    zeros = np.roll(diff, 1) * diff\n",
    "    candidates = np.where(zeros < 0)\n",
    "\n",
    "    maximums = []\n",
    "    for c in candidates[0]:\n",
    "        left = c - 1\n",
    "        right = c + 1\n",
    "        if left < 0:\n",
    "            left = 0\n",
    "        if right >= len(smooth):\n",
    "            right = len(smooth) - 1\n",
    "        if smooth[c] <= smooth[left] and smooth[c] <= smooth[right]:\n",
    "            continue\n",
    "        maximums.append(c)\n",
    "\n",
    "    kernel_opening = np.ones((5, 5), np.uint8)\n",
    "\n",
    "    points_x = []\n",
    "    points_y = []\n",
    "    points_z = []\n",
    "\n",
    "    for i in range(len(maximums)):\n",
    "        if i == 0:\n",
    "            left_thresh = np.min(depth_map)\n",
    "        else:\n",
    "            left_thresh = (bins[maximums[i-1]] + bins[maximums[i]]) / 2\n",
    "        if i == len(maximums) - 1:\n",
    "            right_thresh = np.max(depth_map)\n",
    "        else:\n",
    "            right_thresh = (bins[maximums[i]] + bins[maximums[i+1]]) / 2\n",
    "    \n",
    "        mask = cv2.inRange(depth_map, int(left_thresh), int(right_thresh))\n",
    "        \n",
    "        mask = cv2.erode(mask, kernel_opening, iterations=1)\n",
    "        mask = cv2.dilate(mask, kernel_opening, iterations=1)\n",
    "        \n",
    "        numLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask.astype(np.uint8))\n",
    "        component_areas = stats[:, cv2.CC_STAT_AREA]\n",
    "\n",
    "        for j in range(1, numLabels):\n",
    "            if (component_areas[j] < 200 or component_areas[j] > 20000):\n",
    "                continue\n",
    "            points_z.append(np.mean(depth_map[np.where(labels == j)]))\n",
    "            points_x.append(centroids[j, 0])\n",
    "            points_y.append(centroids[j, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(bins[:-1], smooth)\n",
    "    plt.scatter(bins[maximums], smooth[maximums], color='r')\n",
    "    plt.show()\n",
    "\n",
    "    x, y = np.meshgrid(range(depth_map.shape[1]), range(depth_map.shape[0]))\n",
    "\n",
    "    coords = np.indices(depth_map.shape)\n",
    "    A = np.column_stack((points_x, points_y, np.ones_like(points_x)))\n",
    "    coefficients, residuals, _, _ = np.linalg.lstsq(A, points_z, rcond=None)\n",
    "    a, b, c = coefficients\n",
    "    z_plane = a*coords[0] + b*coords[1] + c\n",
    "\n",
    "    image = cv2.resize(im, (depth_map.shape[1], depth_map.shape[0]))\n",
    "    plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(x, y, depth_map, facecolors=image/255.0, alpha=0.1, rstride=1, cstride=1, cmap=cm.viridis, linewidth=0, antialiased=False, shade=False)\n",
    "    ax.plot_surface(x, y, z_plane, alpha=0.5, color='r')\n",
    "    ax.scatter3D(points_x, points_y, points_z, c=points_z, cmap='Reds')\n",
    "    ax.view_init(azim=45, elev=70)\n",
    "    plt.show()\n",
    "\n",
    "find_plane_with_threshold(im, depth_map, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_derivative(im, depth_map, sigma):\n",
    "    print('shape', depth_map.shape)\n",
    "    print(depth_map.shape[0] * depth_map.shape[1])\n",
    "    rangee = np.max(depth_map) - np.min(depth_map)\n",
    "    hist, bins = np.histogram(depth_map, bins=int(rangee))\n",
    "\n",
    "    kernel = gauss(sigma)\n",
    "\n",
    "    smooth = np.convolve(hist, kernel, mode='same')\n",
    "    diff = np.roll(smooth, -1) - smooth \n",
    "    zeros = np.roll(diff, 1) * diff\n",
    "    candidates = np.where(zeros < 0)\n",
    "\n",
    "    threshold = depth_map.shape[0] * depth_map.shape[1] * 0.001\n",
    "    print(threshold)\n",
    "\n",
    "    maximums = []\n",
    "    for c in candidates[0]:\n",
    "        left = c - 1\n",
    "        right = c + 1\n",
    "        if left < 0:\n",
    "            left = 0\n",
    "        if right >= len(smooth):\n",
    "            right = len(smooth) - 1\n",
    "        if smooth[c] <= smooth[left] and smooth[c] <= smooth[right] or (smooth[c] < threshold):\n",
    "            continue\n",
    "        maximums.append(c)\n",
    "\n",
    "    kernel_opening = np.ones((5, 5), np.uint8)\n",
    "\n",
    "    points_x = []\n",
    "    points_y = []\n",
    "    points_z = []\n",
    "\n",
    "    for i in range(len(maximums)):\n",
    "        if i == 0:\n",
    "            left_thresh = np.min(depth_map)\n",
    "        else:\n",
    "            left_thresh = (bins[maximums[i-1]] + bins[maximums[i]]) / 2\n",
    "        if i == len(maximums) - 1:\n",
    "            right_thresh = np.max(depth_map)\n",
    "        else:\n",
    "            right_thresh = (bins[maximums[i]] + bins[maximums[i+1]]) / 2\n",
    "    \n",
    "        mask = cv2.inRange(depth_map, int(left_thresh), int(right_thresh))\n",
    "        \n",
    "        mask = cv2.erode(mask, kernel_opening, iterations=1)\n",
    "        mask = cv2.dilate(mask, kernel_opening, iterations=1)\n",
    "        plt.imshow(mask)\n",
    "        plt.show()\n",
    "        \n",
    "        numLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask.astype(np.uint8))\n",
    "        component_areas = stats[:, cv2.CC_STAT_AREA]\n",
    "\n",
    "        for j in range(1, numLabels):\n",
    "            if (component_areas[j] < 200 or component_areas[j] > 20000):\n",
    "                continue\n",
    "            points_z.append(np.mean(depth_map[np.where(labels == j)]))\n",
    "            points_x.append(centroids[j, 0])\n",
    "            points_y.append(centroids[j, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(bins[:-1], smooth)\n",
    "    plt.scatter(bins[maximums], smooth[maximums], color='r')\n",
    "    plt.show()\n",
    "\n",
    "    x, y = np.meshgrid(range(depth_map.shape[1]), range(depth_map.shape[0]))\n",
    "\n",
    "    coords = np.indices(depth_map.shape)\n",
    "    A = np.column_stack((points_x, points_y, np.ones_like(points_x)))\n",
    "    coefficients, residuals, _, _ = np.linalg.lstsq(A, points_z, rcond=None)\n",
    "    a, b, c = coefficients\n",
    "    z_plane = a*coords[0] + b*coords[1] + c\n",
    "\n",
    "    image = cv2.resize(im, (depth_map.shape[1], depth_map.shape[0]))\n",
    "    plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(x, y, depth_map, facecolors=image/255.0, alpha=0.1, rstride=1, cstride=1, cmap=cm.viridis, linewidth=0, antialiased=False, shade=False)\n",
    "    ax.plot_surface(x, y, z_plane, alpha=0.5, color='r')\n",
    "    ax.scatter3D(points_x, points_y, points_z, c=points_z, cmap='Reds')\n",
    "    ax.view_init(azim=45, elev=70)\n",
    "    plt.show()\n",
    "\n",
    "test_derivative(im, depth_map, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_plane_with_depth_segmentation(im, depth_map):\n",
    "    print('shape', depth_map.shape)\n",
    "    print(depth_map.shape[0] * depth_map.shape[1])\n",
    "    rangee = np.max(depth_map) - np.min(depth_map)\n",
    "    hist, bins = np.histogram(depth_map, bins=int(rangee))\n",
    "\n",
    "    point_threshold = depth_map.shape[0] * depth_map.shape[1] / 100\n",
    "    \n",
    "    kernel_opening = np.ones((5, 5), np.uint8)\n",
    "    \n",
    "    summ = 0\n",
    "    left_thresh = bins[0]\n",
    "\n",
    "    points_x = []\n",
    "    points_y = []\n",
    "    points_z = []\n",
    "\n",
    "    for i, h in enumerate(hist, start=1):\n",
    "        summ += h\n",
    "        if summ >= point_threshold:\n",
    "            print(summ)\n",
    "            right_thresh = bins[i]\n",
    "            mask = cv2.inRange(depth_map, int(left_thresh), int(right_thresh))\n",
    "            mask = cv2.erode(mask, kernel_opening, iterations=1)\n",
    "            mask = cv2.dilate(mask, kernel_opening, iterations=1)\n",
    "            plt.imshow(mask)\n",
    "            plt.show()\n",
    "            \n",
    "            numLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask.astype(np.uint8))\n",
    "\n",
    "            for j in range(1, numLabels):\n",
    "                points_z.append(np.mean(depth_map[np.where(labels == j)]))\n",
    "                points_x.append(centroids[j, 0])\n",
    "                points_y.append(centroids[j, 1])\n",
    "\n",
    "            summ = 0\n",
    "            left_thresh = bins[i]\n",
    "\n",
    "    x, y = np.meshgrid(range(depth_map.shape[1]), range(depth_map.shape[0]))\n",
    "\n",
    "    coords = np.indices(depth_map.shape)\n",
    "    A = np.column_stack((points_x, points_y, np.ones_like(points_x)))\n",
    "    coefficients, residuals, _, _ = np.linalg.lstsq(A, points_z, rcond=None)\n",
    "    a, b, c = coefficients\n",
    "    z_plane = a*coords[0] + b*coords[1] + c\n",
    "\n",
    "    image = cv2.resize(im, (depth_map.shape[1], depth_map.shape[0]))\n",
    "    plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(x, y, depth_map, facecolors=image/255.0, alpha=0.1, rstride=1, cstride=1, cmap=cm.viridis, linewidth=0, antialiased=False, shade=False)\n",
    "    ax.plot_surface(x, y, z_plane, alpha=0.5, color='r')\n",
    "    ax.scatter3D(points_x, points_y, points_z, c=points_z, cmap='Reds')\n",
    "    ax.view_init(azim=45, elev=70)\n",
    "    plt.show()\n",
    "\n",
    "fit_plane_with_depth_segmentation(im, depth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangee = np.max(depth_map) - np.min(depth_map)\n",
    "depth_norm = ((depth_map - np.min(depth_map)) / rangee)\n",
    "input = depth_map.reshape(-1, 1)\n",
    "x, y = np.meshgrid(np.arange(depth_map.shape[1]), np.arange(depth_map.shape[0]))\n",
    "coords = np.column_stack((x.reshape(-1, 1), y.reshape(-1, 1)))\n",
    "input = np.column_stack((input, coords))\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "kmeans.fit(input)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "labels_2d = labels.reshape(depth_map.shape[0], depth_map.shape[1])\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "plt.imshow(depth_map)\n",
    "plt.show()\n",
    "plt.imshow(labels_2d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangee = np.max(depth_map) - np.min(depth_map)\n",
    "depth_norm = ((depth_map - np.min(depth_map)) / rangee)\n",
    "input = depth_map.reshape(-1, 1)\n",
    "x, y = np.meshgrid(np.arange(depth_map.shape[1]), np.arange(depth_map.shape[0]))\n",
    "coords = np.column_stack((x.reshape(-1, 1), y.reshape(-1, 1)))\n",
    "input = np.column_stack((input, coords))\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "kmeans.fit(input)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "labels_2d = labels.reshape(depth_map.shape[0], depth_map.shape[1])\n",
    "\n",
    "unique_labels = np.unique(labels_2d)\n",
    "centroids = []\n",
    "x_p = []\n",
    "y_p = []\n",
    "z_p = []\n",
    "for label in unique_labels:\n",
    "    indices = np.where(labels_2d == label)\n",
    "    centroid = np.mean(indices, axis=1, dtype=np.int)\n",
    "    zp = np.mean(depth_map[indices])\n",
    "    centroids.append(centroid)\n",
    "    x_p.append(centroid[1])\n",
    "    y_p.append(centroid[0])\n",
    "    z_p.append(zp)\n",
    "    labels_2d[int(centroid[0]) -2 : int(centroid[0]) +2, int(centroid[1]) - 2 : int(centroid[1]) + 2] = label\n",
    "\n",
    "labels_2d_rgb = cv2.cvtColor(labels_2d.astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "\n",
    "A = np.column_stack((x_p, y_p, np.ones_like(x_p)))\n",
    "coefficients, residuals, _, _ = np.linalg.lstsq(A, z_p, rcond=None)\n",
    "a, b, c = coefficients\n",
    "zr = a*x + b*y + c\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "image_plot = cv2.resize(im, (depth_map.shape[1], depth_map.shape[0]))\n",
    "#ax.plot_surface(x, y, zr, alpha=0.5, color='b')\n",
    "ax.plot_surface(x, y, depth_map, facecolors=labels_2d_rgb/255.0, alpha=0.1, rstride=1, cstride=1, cmap=cm.viridis, linewidth=0, antialiased=False, shade=False)\n",
    "ax.scatter(x_p, y_p, z_p, c='r', marker='o')\n",
    "ax.view_init(azim=30, elev=70)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 3):\n",
    "    data = load_images(i)\n",
    "    index = 0\n",
    "    for image in data:\n",
    "        im = load_np_image(image)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(im)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        depth_map = predict_depth(im)\n",
    "        plt.imshow(depth_map)\n",
    "        plt.show()\n",
    "        \n",
    "        print('index', index)\n",
    "        print(voxelize_whole_ratio(depth_map, im.shape[0], im.shape[1]), 'image ratio')\n",
    "        print(voxelize_whole_ratio_no_depth(depth_map, im.shape[0], im.shape[1]), 'image ratio no depth')\n",
    "        print(voxelize_no_back(depth_map, im.shape[0], im.shape[1]), 'no back')\n",
    "        print(voxelize_no_back_no_depth(depth_map, im.shape[0], im.shape[1]), 'no back')\n",
    "      \n",
    "        index += 1\n",
    "    #plot_3d(im, depth_map)\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Load your single-channel image and get the pixel values\n",
    "data = load_images(3)\n",
    "image = load_np_image(data[44])\n",
    "depth_map = predict_depth(image)\n",
    "pixel_values = depth_map.flatten()\n",
    "\n",
    "# Get the x and y coordinates of each pixel\n",
    "height, width = depth_map.shape\n",
    "x_coords, y_coords = np.meshgrid(range(width), range(height))\n",
    "coords = np.column_stack((x_coords.flatten(), y_coords.flatten()))\n",
    "\n",
    "# Define your custom distance metric\n",
    "def custom_distance(p1, p2):\n",
    "    # Euclidean distance between pixel values\n",
    "    value_distance = abs(p1[0] - p2[0])\n",
    "\n",
    "    # Distance between pixel coordinates\n",
    "    coord_distance = np.linalg.norm(p1[1:] - p2[1:])\n",
    "\n",
    "    # Combine the distances (you can adjust the weighting factor)\n",
    "    distance = value_distance + 0.5 * coord_distance\n",
    "\n",
    "    return distance\n",
    "\n",
    "# Perform clustering using DBSCAN\n",
    "epsilon = 8.0\n",
    "min_samples = 8\n",
    "dbscan = DBSCAN(eps=epsilon, min_samples=min_samples, metric=custom_distance)\n",
    "labels = dbscan.fit_predict(np.column_stack((pixel_values, coords)))\n",
    "\n",
    "print(labels)\n",
    "plt.imshow(labels.reshape(height, width))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
